{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "senior-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import SimpleITK as itk\n",
    "from LabelFusion.wrapper import fuse_images\n",
    "\n",
    "import echonet\n",
    "from echonet.datasets import Echo\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from src.utils.torch_utils import TransformDataset, torch_collate\n",
    "from src.utils.echo_utils import get2dPucks\n",
    "from src.utils.camus_validate import cleanupSegmentation\n",
    "from src.transform_utils import generate_2dmotion_field\n",
    "from src.visualization_utils import categorical_dice\n",
    "from src.loss_functions import huber_loss, convert_to_1hot, convert_to_1hot_tensor\n",
    "from src.echonet_dataset import EDESpairs, EchoNetDynamicDataset\n",
    "from src.model.R2plus1D_18_MotionNet import R2plus1D_18_MotionNet\n",
    "# from src.visualization_utils import categorical_dice\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "tic, toc = (time.time, time.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defensive-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_workers = max(4, cpu_count()//2)\n",
    "\n",
    "\n",
    "def worker_init_fn_valid(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    # See here: https://pytorch.org/docs/stable/notes/randomness.html#dataloader\n",
    "    # and the original post of the problem: https://github.com/pytorch/pytorch/issues/5059#issuecomment-817373837\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "\n",
    "def permuter(list1, list2):\n",
    "    for i1 in list1:\n",
    "        for i2 in list2:\n",
    "            yield (i1, i2)\n",
    "            \n",
    "\n",
    "param_trainLoader = {'collate_fn': torch_collate,\n",
    "                     'batch_size': batch_size,\n",
    "                     'num_workers': max(4, cpu_count()//2),\n",
    "                     'worker_init_fn': worker_init_fn}\n",
    "\n",
    "param_testLoader = {'collate_fn': torch_collate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': max(4, cpu_count()//2),\n",
    "                    'worker_init_fn': worker_init_fn}\n",
    "\n",
    "paramLoader = {'train': param_trainLoader,\n",
    "               'valid': param_testLoader,\n",
    "               'test':  param_testLoader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expanded-suspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 14.19it/s]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.72it/s]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.84it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"fold_indexes/stanford_valid_sampled_indices\", \"rb\") as infile:\n",
    "    valid_mask = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "full_dataset = EchoNetDynamicDataset(split='val', clip_length=\"full\", subset_indices=valid_mask, period=1)\n",
    "test_dataset = EchoNetDynamicDataset(split='test', clip_length=\"full\", raise_for_es_ed=False, period=1)\n",
    "random_test_dataset = EchoNetDynamicDataset(split='test', clip_length=32, raise_for_es_ed=True, period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loved-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_possible_start_points(ed_index, es_index, video_length, clip_length):\n",
    "    assert es_index - ed_index > 0, \"not a ED to ES clip pair\"\n",
    "    possible_shift = clip_length - (es_index - ed_index)\n",
    "    allowed_right = video_length - es_index\n",
    "    if allowed_right < possible_shift:\n",
    "        return np.arange(ed_index - possible_shift + 1, video_length - clip_length + 1)\n",
    "    if possible_shift < 0:\n",
    "        return np.array([ed_index])\n",
    "    elif ed_index < possible_shift:\n",
    "        return np.arange(ed_index + 1)\n",
    "    else:\n",
    "        return np.arange(ed_index - possible_shift + 1, ed_index + 1)\n",
    "    \n",
    "\n",
    "# from queue import SimpleQueue as squeue\n",
    "def EDESpairs(diastole, systole):\n",
    "    dframes = np.sort(np.array(diastole))\n",
    "    sframes = np.sort(np.array(systole))\n",
    "    clips = []\n",
    "    \n",
    "    inds = np.searchsorted(dframes, sframes, side='left')\n",
    "    for i, sf in enumerate(sframes):\n",
    "        if inds[i] == 0: # no prior diastolic frames for this sf\n",
    "            continue\n",
    "        best_df = diastole[inds[i]-1] # diastole frame nearest this sf.\n",
    "        if len(clips) == 0 or best_df != clips[-1][0]:\n",
    "            clips.append((best_df, sf))\n",
    "            \n",
    "    return clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acknowledged-cardiff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2+1D MotionNet has 31575731 parameters.\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"save_models/R2plus1DMotionSegNet_model_tmp.pth\"\n",
    "\n",
    "model = torch.nn.DataParallel(R2plus1D_18_MotionNet())\n",
    "model.to(\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "model.load_state_dict(torch.load(model_save_path)[\"model\"])\n",
    "print(f'R2+1D MotionNet has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.')\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-respect",
   "metadata": {},
   "source": [
    "### 1. Function for computing EF using fused segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fuzzy-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_to_consecutive_clips(video, clip_length=32, interpolate_last=False):\n",
    "    source_video = video.copy()\n",
    "    video_length = video.shape[1]\n",
    "    left = video_length % clip_length\n",
    "    if left != 0 and interpolate_last:\n",
    "        source_video = torch.Tensor(source_video).unsqueeze(0)\n",
    "        source_video = F.interpolate(source_video, size=(int(np.round(video_length / clip_length) * clip_length), 112, 112),\n",
    "                                     mode=\"trilinear\", align_corners=False)\n",
    "        source_video = source_video.squeeze(0).squeeze(0)\n",
    "        source_video = source_video.numpy()\n",
    "    \n",
    "    videos = np.empty(shape=(1, 3, clip_length, 112, 112))\n",
    "\n",
    "    for start in range(0, int(clip_length * np.round(video_length / clip_length)), clip_length):\n",
    "        one_clip = source_video[:, start: start + clip_length]\n",
    "        one_clip = np.expand_dims(one_clip, 0)\n",
    "        videos = np.concatenate([videos, one_clip])\n",
    "    return videos[1:]\n",
    "\n",
    "\n",
    "def segment_a_video_with_fusion(video, interpolate_last=True, step=1, num_clips=10, \n",
    "                                fuse_method=\"simple\", class_list=[0, 1]):\n",
    "    if video.shape[1] < 32 + num_clips * step:\n",
    "        num_clips = (video.shape[1] - 32) // step\n",
    "    if num_clips < 0:\n",
    "        print(\"Video is too short\")\n",
    "        num_clips = 1\n",
    "    all_consecutive_clips = []\n",
    "\n",
    "    for shift_dis in range(0, num_clips * step, step):\n",
    "        shifted_video = video[:, shift_dis:]\n",
    "        consecutive_clips = divide_to_consecutive_clips(shifted_video, interpolate_last=interpolate_last)\n",
    "        all_consecutive_clips.append(consecutive_clips)\n",
    "\n",
    "    all_consecutive_clips = np.array(all_consecutive_clips)\n",
    "    all_segmentations = []\n",
    "\n",
    "    for i in range(len(all_consecutive_clips)):\n",
    "        consecutive_clips = all_consecutive_clips[i]\n",
    "        segmentation_outputs = np.empty(shape=(1, 2, 32, 112, 112))\n",
    "\n",
    "        for i in range(consecutive_clips.shape[0]):\n",
    "            one_clip = np.expand_dims(consecutive_clips[i], 0)\n",
    "            segmentation_output, motion_output = model(torch.Tensor(one_clip))\n",
    "            segmentation_output = F.softmax(segmentation_output, 1)\n",
    "            segmentation_outputs = np.concatenate([segmentation_outputs, segmentation_output.cpu().detach().numpy()])\n",
    "        segmentation_outputs = segmentation_outputs[1:]\n",
    "\n",
    "        all_segmentations.append(segmentation_outputs)\n",
    "\n",
    "    for i in range(len(all_segmentations)):\n",
    "        all_segmentations[i] = all_segmentations[i].transpose([1, 0, 2, 3, 4])\n",
    "        all_segmentations[i] = all_segmentations[i].reshape(2, -1, 112, 112)\n",
    "\n",
    "    all_interpolated_segmentations = []\n",
    "    for i in range(0, len(all_consecutive_clips)):\n",
    "        video_clip = video[:, i * step:]\n",
    "        if interpolate_last and (video_clip.shape[1] % 32 != 0):\n",
    "            interpolated_segmentations = torch.Tensor(all_segmentations[i]).unsqueeze(0)\n",
    "            interpolated_segmentations = F.interpolate(interpolated_segmentations, size=(video_clip.shape[1], 112, 112), \n",
    "                                                       mode=\"trilinear\", align_corners=False)\n",
    "            interpolated_segmentations = interpolated_segmentations.squeeze(0).numpy()\n",
    "            all_interpolated_segmentations.append(np.argmax(interpolated_segmentations, 0))\n",
    "        else:\n",
    "            all_interpolated_segmentations.append(np.argmax(all_segmentations[i], 0))\n",
    "\n",
    "    fused_segmentations = [all_interpolated_segmentations[0][0]]\n",
    "\n",
    "    for i in range(1, video.shape[1]):\n",
    "        if step - 1 < i:\n",
    "            images_to_fuse = []\n",
    "            for index in range(min(i, len(all_interpolated_segmentations))):\n",
    "                if i - index * step < 0:\n",
    "                    break\n",
    "                images_to_fuse.append(itk.GetImageFromArray(all_interpolated_segmentations[index][i - index * step].astype(\"uint8\"),\n",
    "                                                            isVector=False))\n",
    "            if len(images_to_fuse) <= 1:\n",
    "                fused_segmentations.append(itk.GetArrayFromImage(images_to_fuse[0]))\n",
    "            else:\n",
    "                fused_image = fuse_images(images_to_fuse, fuse_method, class_list=class_list)\n",
    "                # If using SIMPLE, the fused image might be in type \"float\"\n",
    "                # So convert it to uint\n",
    "                fused_segmentations.append(itk.GetArrayFromImage(fused_image).astype(\"uint8\"))\n",
    "\n",
    "    fused_segmentations = np.array(fused_segmentations)\n",
    "    \n",
    "    return fused_segmentations\n",
    "\n",
    "\n",
    "def compute_ef_using_putative_clips(fused_segmentations, test_pat_index):\n",
    "    size = np.sum(fused_segmentations, axis=(1, 2)).ravel()\n",
    "    _05cut, _85cut, _95cut = np.percentile(size, [5, 85, 95]) \n",
    "\n",
    "    trim_min = _05cut\n",
    "    trim_max = _95cut\n",
    "    trim_range = trim_max - trim_min\n",
    "    systole = find_peaks(-size, distance=20, prominence=(0.50 * trim_range))[0]\n",
    "    diastole = find_peaks(size, distance=20, prominence=(0.50 * trim_range))[0]\n",
    "\n",
    "    # keep only real diastoles..\n",
    "    diastole = [x for x in diastole if size[x] >= _85cut]\n",
    "    # Add first frame\n",
    "    if np.mean(size[:3]) >= _85cut:\n",
    "        diastole = [0] + diastole\n",
    "    diastole = np.array(diastole)\n",
    "\n",
    "    clip_pairs = EDESpairs(diastole, systole)\n",
    "\n",
    "    one_array_of_segmentations = fused_segmentations.reshape(-1, 112, 112)\n",
    "\n",
    "    predicted_efs = []\n",
    "\n",
    "    for i in range(len(clip_pairs)):\n",
    "        output_ED = one_array_of_segmentations[clip_pairs[i][0]]\n",
    "        output_ES = one_array_of_segmentations[clip_pairs[i][1]]\n",
    "        \n",
    "        length_ed, radius_ed = get2dPucks((output_ED == 1).astype('int'), (1.0, 1.0))\n",
    "        length_es, radius_es = get2dPucks((output_ES == 1).astype('int'), (1.0, 1.0))\n",
    "\n",
    "        edv = np.sum(((np.pi * radius_ed * radius_ed) * length_ed / len(radius_ed)))\n",
    "        esv = np.sum(((np.pi * radius_es * radius_es) * length_es / len(radius_es)))\n",
    "\n",
    "        ef_predicted = (edv - esv) / edv * 100\n",
    "        \n",
    "        if ef_predicted < 0:\n",
    "            print(\"Negative EF at patient:{:04d}\".format(test_pat_index))\n",
    "            continue\n",
    "\n",
    "        predicted_efs.append(ef_predicted)\n",
    "\n",
    "    return predicted_efs\n",
    "\n",
    "\n",
    "def compute_ef_using_reported_clip(segmentations, ed_index, es_index):\n",
    "    output_ED = segmentations[ed_index]\n",
    "    output_ES = segmentations[es_index]\n",
    "\n",
    "    lv_ed_dice = categorical_dice((output_ED), ed_label, 1)\n",
    "    lv_es_dice = categorical_dice((output_ES), es_label, 1)\n",
    "\n",
    "    length_ed, radius_ed = get2dPucks((output_ED == 1).astype('int'), (1.0, 1.0))\n",
    "    length_es, radius_es = get2dPucks((output_ES == 1).astype('int'), (1.0, 1.0))\n",
    "    \n",
    "    edv = np.sum(((np.pi * radius_ed * radius_ed) * length_ed / len(radius_ed)))\n",
    "    esv = np.sum(((np.pi * radius_es * radius_es) * length_es / len(radius_es)))\n",
    "\n",
    "    ef_predicted = (edv - esv) / edv * 100\n",
    "    \n",
    "    return ef_predicted, lv_ed_dice, lv_es_dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-humidity",
   "metadata": {},
   "source": [
    "### 2. Compute the Ejection Fraction for all Test Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "collected-imaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dynamic37-labelfusion/lib/python3.7/site-packages/ipykernel_launcher.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot identify clips at patient:0095\n",
      "Cannot identify clips at patient:0294\n",
      "Cannot identify clips at patient:0390\n",
      "Cannot identify clips at patient:0573\n",
      "Cannot identify clips at patient:0612\n",
      "Cannot identify clips at patient:1061\n",
      "Cannot identify clips at patient:1215\n",
      "Used time = 68 mins 46 secs\n"
     ]
    }
   ],
   "source": [
    "patient_filename = []\n",
    "\n",
    "EF_list = []\n",
    "true_EF_list = []\n",
    "mean_EF_list = []\n",
    "\n",
    "lv_ed_dice = []\n",
    "lv_es_dice = []\n",
    "\n",
    "num_clips = 5\n",
    "step = 1\n",
    "interpolate_last = True\n",
    "fuse_method = \"simple\"\n",
    "class_list = [0, 1]\n",
    "# class_list = None\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    test_pat_index = i\n",
    "    try:\n",
    "        video, (filename, EF, es_clip_index, ed_clip_index, es_index, ed_index, es_frame, ed_frame, es_label, ed_label) = test_dataset[test_pat_index]\n",
    "    except:\n",
    "        print(\"Get exception when trying to read the video from patient:{:04d}\".format(i))\n",
    "        continue\n",
    "\n",
    "    if test_pat_index == 1053:\n",
    "        video = video[:, :80]\n",
    "    \n",
    "    segmentations = segment_a_video_with_fusion(video, interpolate_last=interpolate_last, \n",
    "                                                step=step, num_clips=num_clips,\n",
    "                                                fuse_method=fuse_method, class_list=class_list)\n",
    "\n",
    "    predicted_efs = compute_ef_using_putative_clips(segmentations, test_pat_index=test_pat_index)\n",
    "    \n",
    "    _, ed_dice, es_dice = compute_ef_using_reported_clip(segmentations, ed_index, es_index)\n",
    "\n",
    "    lv_ed_dice.append(ed_dice)\n",
    "    lv_es_dice.append(es_dice)\n",
    "    \n",
    "    if len(predicted_efs) == 0:\n",
    "        print(\"Cannot identify clips at patient:{:04d}\".format(test_pat_index))\n",
    "        continue\n",
    "    \n",
    "    if np.isnan(np.nanmean(predicted_efs)):\n",
    "        print(\"Cannot identify clips at patient:{:04d}\".format(test_pat_index))\n",
    "        continue\n",
    "               \n",
    "    EF_list.append(predicted_efs)\n",
    "    true_EF_list.append(EF)\n",
    "    mean_EF_list.append(np.nanmean(predicted_efs))\n",
    "    patient_filename.append(filename[:-4])\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(\"Used time = {:.0f} mins {:.0f} secs\".format((end - start) // 60, (end - start) % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-cloud",
   "metadata": {},
   "source": [
    "### Label fusion of 5 clips with step 1 using full video segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "formed-cookie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error (standard deviation):  5.2519 (4.4517) %\n",
      "Median absolute error:  4.1038 %\n",
      "Bias +- 1.96 x std:  -2.0939 +- 12.8549\n",
      "Percentile of mae 50%: 4.1038  75%: 7.3610  95%: 13.8551\n"
     ]
    }
   ],
   "source": [
    "errors = np.array(np.array(true_EF_list) - np.array(mean_EF_list))\n",
    "abs_errors = abs(errors)\n",
    "\n",
    "print(\"Mean absolute error (standard deviation):  {:.4f} ({:.4f}) %\".format(np.mean(abs_errors), np.std(abs_errors)))\n",
    "print(\"Median absolute error:  {:.4f} %\".format(np.median(abs_errors)))\n",
    "print(\"Bias +- 1.96 x std:  {:.4f} +- {:.4f}\".format(np.mean(errors), 1.96 * np.std(errors)))\n",
    "print(\"Percentile of mae 50%: {:6.4f}  75%: {:6.4f}  95%: {:6.4f}\".format(np.percentile(abs_errors, 50), np.percentile(abs_errors, 75),\n",
    "                                                                    np.percentile(abs_errors, 95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "configured-chaos",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ED 0.9354 (0.0324)\n",
      "Median ED 0.9421\n",
      "Average ES 0.9073 (0.0515)\n",
      "Median ES 0.9189\n"
     ]
    }
   ],
   "source": [
    "print(\"Average ED {:.4f} ({:.4f})\".format(np.mean(lv_ed_dice), np.std(lv_ed_dice)))\n",
    "print(\"Median ED {:.4f}\".format(np.median(lv_ed_dice)))\n",
    "print(\"Average ES {:.4f} ({:.4f})\".format(np.mean(lv_es_dice), np.std(lv_es_dice)))\n",
    "print(\"Median ES {:.4f}\".format(np.median(lv_es_dice)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-anxiety",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
